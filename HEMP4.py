import os
import gc
import uuid
import tempfile
import logging
from typing import List, Dict, Optional
from rag_code import Transcribe, EmbedData, QdrantVDB_QB, Retriever, RAG
import streamlit as st
from textblob import TextBlob
from fpdf import FPDF
from dotenv import load_dotenv
import json
from datetime import datetime
import pandas as pd
from io import BytesIO

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Add custom CSS and JavaScript
st.markdown("""
    <style>
    /* General styling */
    .stApp {
        background: #000000;
        color: #e0e0e0;
        font-family: 'Arial', sans-serif;
    }
    
    /* Title */
    h1 {
        color: #00d4ff;
        text-shadow: 0 0 10px rgba(0,212,255,0.5);
        animation: fadeIn 1s ease-in;
    }
    
    /* Sidebar */
    .sidebar .sidebar-content {
        background: rgba(255,255,255,0.1);
        border-radius: 10px;
        padding: 15px;
        backdrop-filter: blur(5px);
    }
    
    /* Buttons */
    .stButton>button {
        background: linear-gradient(45deg, #00d4ff, #007bff);
        color: white;
        border: none;
        border-radius: 5px;
        padding: 8px 15px;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,212,255,0.4);
    }
    
    /* Tabs */
    .stTabs [data-baseweb="tab"] {
        background: rgba(255,255,255,0.1);
        border-radius: 5px 5px 0 0;
        color: #e0e0e0;
        transition: all 0.3s ease;
        padding: 5px 10px;
        min-width: 50px;
        text-align: center;
    }
    .stTabs [data-baseweb="tab"][aria-selected="true"] {
        background: #007bff;
        color: white;
    }
    
    /* Chat messages */
    .stChatMessage {
        background: rgba(255,255,255,0.05);
        border-radius: 5px;
        padding: 10px;
        margin: 5px 0;
        animation: slideIn 0.3s ease;
    }
    
    /* Dataframe */
    .stDataFrame {
        border: 1px solid rgba(255,255,255,0.2);
        border-radius: 5px;
        background: rgba(255,255,255,0.05);
    }
    
    /* Animations */
    @keyframes fadeIn {
        from { opacity: 0; }
        to { opacity: 1; }
    }
    
    @keyframes slideIn {
        from { transform: translateX(-20px); opacity: 0; }
        to { transform: translateX(0); opacity: 1; }
    }
    
    /* Spinner */
    .stSpinner {
        color: #00d4ff !important;
    }
    </style>
    
    <script>
    // Add hover animation to chat messages
    document.addEventListener('DOMContentLoaded', () => {
        const messages = document.querySelectorAll('.stChatMessage');
        messages.forEach(msg => {
            msg.addEventListener('mouseenter', () => {
                msg.style.background = 'rgba(255,255,255,0.1)';
                msg.style.transition = 'background 0.3s ease';
            });
            msg.addEventListener('mouseleave', () => {
                msg.style.background = 'rgba(255,255,255,0.05)';
            });
        });
    });
    
    // Smooth scroll for chat
    const chatContainer = document.querySelector('.stChatMessage')?.parentElement;
    if (chatContainer) {
        chatContainer.style.scrollBehavior = 'smooth';
    }
    </script>
""", unsafe_allow_html=True)

# Singleton embedding model with caching
@st.cache_resource
def get_embed_model():
    logger.info("Initializing embedding model: BAAI/bge-large-en-v1.5")
    return EmbedData(embed_model_name="BAAI/bge-large-en-v1.5", batch_size=32)

class EnhancedTranscribe(Transcribe):
    def __init__(self, api_key: str):
        super().__init__(api_key)
        self.sentiment_analyzer = TextBlob
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        blob = self.sentiment_analyzer(text)
        return {"polarity": blob.sentiment.polarity, "subjectivity": blob.sentiment.subjectivity}

    def transcribe_audio(self, audio_path: str, language: Optional[str] = None) -> List[Dict]:
        transcripts = super().transcribe_audio(audio_path, language)
        for t in transcripts:
            sentiment = self.analyze_sentiment(t["text"])
            t.update({
                "sentiment": sentiment,
                "timestamp": datetime.now().isoformat(),
                "word_count": len(t["text"].split())
            })
        return transcripts

class AudioRAGManager:
    def __init__(self, collection_name: str, api_key: str):
        self.collection_name = collection_name
        self.transcriber = EnhancedTranscribe(api_key=api_key)
        self.embeddata = get_embed_model()
        self.vector_db = QdrantVDB_QB(collection_name=collection_name, batch_size=512)
        self.retriever = None
        self.rag = None
        self.api_key = api_key

    def process_audio(self, audio_path: str, language: str = "en") -> List[Dict]:
        try:
            transcripts = self.transcriber.transcribe_audio(audio_path, language)
            documents = [f"{t['speaker']}: {t['text']} (Sentiment: {t['sentiment']['polarity']:.2f})" 
                        for t in transcripts]
            
            self.embeddata.embed(documents)
            self.vector_db.define_client()
            self.vector_db.create_collection()
            self.vector_db.ingest_data(self.embeddata)
            
            self.retriever = Retriever(vector_db=self.vector_db, embeddata=self.embeddata)
            self.rag = RAG(retriever=self.retriever, llm_name="DeepSeek-R1-Distill-Llama-70B")
            
            return transcripts
        except Exception as e:
            logger.error(f"Processing error: {e}")
            raise

    def export_to_pdf(self, transcripts: List[Dict]) -> BytesIO:
        pdf = FPDF()
        pdf.add_page()
        pdf.set_font("Arial", size=12)
        pdf.cell(200, 10, txt="Audio Transcript Analysis", ln=True, align="C")
        pdf.ln(10)
        
        for t in transcripts:
            pdf.multi_cell(0, 10, f"[{t['timestamp']}] {t['speaker']}: {t['text']}")
            pdf.multi_cell(0, 10, f"Sentiment: P={t['sentiment']['polarity']:.2f}, S={t['sentiment']['subjectivity']:.2f}")
            pdf.ln(5)
        
        return BytesIO(pdf.output(dest='S').encode('latin-1'))

    def export_to_json(self, transcripts: List[Dict]) -> BytesIO:
        return BytesIO(json.dumps(transcripts, indent=2).encode('utf-8'))

    def get_statistics(self, transcripts: List[Dict]) -> Dict:
        if not transcripts:
            return {}
        word_counts = [t["word_count"] for t in transcripts]
        polarities = [t["sentiment"]["polarity"] for t in transcripts]
        return {
            "total_segments": len(transcripts),
            "avg_word_count": sum(word_counts) / len(word_counts),
            "avg_sentiment": sum(polarities) / len(polarities),
            "speakers": len(set(t["speaker"] for t in transcripts))
        }

def format_analysis(text: str, sentiment: float) -> str:
    return f"""
    ### Sentiment Analysis
    **Sentiment Score**: {sentiment:.2f} (Slightly {'Negative' if sentiment < 0 else 'Positive' if sentiment > 0 else 'Neutral'})

    #### Interpretation
    {text}

    #### Summary
    Speaker B expresses a complex mix of emotions centered around dependency on another person. They feel significantly impacted, showing a bittersweet attachment with a hint of feeling trapped or changed, reflected in the slightly negative sentiment.
    """

def run_enhanced_app():
    # Initialize session state
    if "id" not in st.session_state:
        st.session_state.id = uuid.uuid4()
        st.session_state.file_cache = {}
        st.session_state.messages = []
        st.session_state.transcripts = []
        st.session_state.history = []
        st.session_state.current_file = None
        st.session_state.summary = None

    session_id = st.session_state.id
    manager = AudioRAGManager(collection_name=f"enhanced_audio_{session_id}", 
                            api_key=os.getenv("ASSEMBLYAI_API_KEY"))

    # Main title and layout
    st.title("Audio RAG Analyzer")
    st.markdown(f"**Session ID**: {session_id.hex[:8]}")

    # Sidebar for controls
    with st.sidebar:
        st.header("Audio Processing Controls")
        uploaded_file = st.file_uploader("Upload Audio File", type=["mp3", "wav", "m4a"])
        
        lang_map = {"English": "en", "French": "fr", "Spanish": "es", "German": "de"}
        language = st.selectbox("Language", list(lang_map.keys()), index=0)
        export_format = st.multiselect("Export Formats", ["PDF", "JSON"], default=["PDF"])
        summarize = st.checkbox("Generate Summary")
        save_history = st.checkbox("Save Chat History")
        if st.button("Clear Session"):
            reset_chat()
            st.success("Session cleared!")

    # Main content area with icons instead of text
    tab1, tab2, tab3 = st.tabs(["‚öôÔ∏è", "üìú", "üí¨"])

    with tab1:
        if uploaded_file:
            with st.spinner("Processing audio..."):
                try:
                    with tempfile.TemporaryDirectory() as temp_dir:
                        file_path = os.path.join(temp_dir, uploaded_file.name)
                        with open(file_path, "wb") as f:
                            f.write(uploaded_file.getvalue())
                        
                        transcripts = manager.process_audio(file_path, lang_map[language])
                        st.session_state.transcripts = transcripts
                        st.session_state.file_cache[uploaded_file.name] = manager
                        st.session_state.current_file = uploaded_file.name

                        # Audio player
                        st.audio(file_path, format=f"audio/{uploaded_file.name.split('.')[-1]}")

                        # Export options
                        col1, col2 = st.columns(2)
                        if "PDF" in export_format:
                            with col1:
                                pdf_data = manager.export_to_pdf(transcripts)
                                st.download_button("Download PDF", pdf_data, f"{uploaded_file.name}_transcript.pdf", "application/pdf")
                        if "JSON" in export_format:
                            with col2:
                                json_data = manager.export_to_json(transcripts)
                                st.download_button("Download JSON", json_data, f"{uploaded_file.name}_transcript.json", "application/json")

                        # Summary
                        if summarize:
                            full_text = "\n".join([t['text'] for t in transcripts])
                            st.session_state.summary = manager.rag.summarize(full_text)
                            st.subheader("Summary")
                            st.write(st.session_state.summary)

                        st.success("Audio processed successfully!")
                except Exception as e:
                    st.error(f"Error processing audio: {str(e)}")

        # Statistics
        if st.session_state.transcripts:
            stats = manager.get_statistics(st.session_state.transcripts)
            st.subheader("Audio Statistics")
            st.json(stats)

    with tab2:
        if st.session_state.transcripts:
            st.subheader("Transcript")
            df = pd.DataFrame(st.session_state.transcripts)
            df_display = df[["timestamp", "speaker", "text", "sentiment"]]
            df_display["sentiment"] = df_display["sentiment"].apply(lambda x: f"P={x['polarity']:.2f}, S={x['subjectivity']:.2f}")
            st.dataframe(df_display, use_container_width=True)

    with tab3:
        # Chat interface
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("Ask about the audio..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                try:
                    file_key = st.session_state.current_file
                    manager = st.session_state.file_cache.get(file_key)
                    
                    if manager and manager.rag:
                        full_response = ""
                        message_placeholder = st.empty()
                        streaming_response = manager.rag.query(prompt)
                        
                        for chunk in streaming_response:
                            try:
                                # Handle potential different response formats
                                if hasattr(chunk, 'raw') and "choices" in chunk.raw and chunk.raw["choices"]:
                                    new_text = chunk.raw["choices"][0]["delta"].get("content", "")
                                elif hasattr(chunk, 'content'):
                                    new_text = chunk.content
                                else:
                                    new_text = str(chunk)  # Fallback to string representation
                                
                                full_response += new_text
                                message_placeholder.markdown(full_response + "‚ñå")
                            except Exception as e:
                                logger.error(f"Chunk processing error: {e}")
                                full_response += "[Error processing chunk]"
                                break
                        
                        # Format specific response if it matches your example
                        if "Speaker B" in prompt.lower():
                            analysis_text = """
                            Speaker B expresses a mix of emotions about becoming dependent on someone else. 
                            They feel this person has significantly impacted their life, as seen in phrases like 
                            "look what you've done to me" and "look what you've done now." The repetition of 
                            "Baby I'll never leave if you keep holding me this way" suggests a clingy desire 
                            to stay, possibly feeling trapped. The phrase "maybe you got me side down" might 
                            imply feeling down or deceived, though its exact meaning is unclear. 
                            """
                            full_response = format_analysis(analysis_text, -0.16)
                        
                        message_placeholder.markdown(full_response)
                        
                        if save_history:
                            st.session_state.history.append({
                                "timestamp": datetime.now().isoformat(),
                                "query": prompt,
                                "response": full_response,
                                "file": file_key
                            })
                    else:
                        st.write("Please upload an audio file first.")
                    
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                except Exception as e:
                    st.error(f"Chat error: {str(e)}")
                    logger.error(f"Chat processing error: {e}")

        # History export
        if st.session_state.history and save_history:
            if st.button("Export Chat History"):
                history_json = json.dumps(st.session_state.history, indent=2)
                st.download_button(
                    label="Download History",
                    data=history_json,
                    file_name=f"chat_history_{session_id.hex[:8]}.json",
                    mime="application/json"
                )

def reset_chat():
    st.session_state.messages = []
    st.session_state.transcripts = []
    st.session_state.current_file = None
    st.session_state.summary = None
    st.session_state.history = []
    gc.collect()

if __name__ == "__main__":
    run_enhanced_app()